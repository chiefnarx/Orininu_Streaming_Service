üé∂ **Orininu Streaming Service Database**

This project focuses on creating, managing, and migrating the database for the Orininu Streaming Service. Starting with **PostgreSQL**, the data model was designed and structured, generated synthetic data using **Python**, and later transitioned the system to **MongoDB Atlas** for better flexibility and scalability.

The process also involved automating data synchronization using **Task Scheduler** to ensure PostgreSQL data remained up-to-date in MongoDB.

üìå **Project Overview**

    This project establishes the Orininu Streaming Service database from scratch, covering: 

    ‚úÖ PostgreSQL database creation.
    
    ‚úÖ Data generation using Python.
    
    ‚úÖ Full migration to MongoDB Atlas.

‚öôÔ∏è **Technologies Used**
    
    ‚è∫ PostgreSQL: Relational Database Setup.
    
    ‚è∫ SQL Schema Design: Defining Tables & Relationships.
    
    ‚è∫ Python (pandas, faker): Synthetic Data Generation.
    
    ‚è∫ MongoDB Atlas: Database Migration.
    
    ‚è∫ MongoDB Shell (mongosh): Database Management.
    
    ‚è∫ MongoDB Database Tools (mongoimport): Importing Data into MongoDB.
    
    ‚è∫ Windows Task Scheduler: Scheduling and Importing Data into MongoDB.
 
üîÑ **Full Database Setup Workflow**

    1. Generate Synthetic Data Using Python:
    
        ‚Ä¢ Used faker & pandas to create realistic user profiles, songs, artists, albums, etc.
        
        ‚Ä¢ Stored and exported generated data in a CSV format.

    2. Design & Create PostgreSQL Database:
    
        ‚Ä¢ Defined schema with DDL.sql.
        
        ‚Ä¢ Created tables for users, artists, albums, songs, user_interaction, playlists, playlist_songs.
        
        ‚Ä¢ Established foreign key relationships.
    
    3. Populate PostgreSQL with Generated Data:
    
        ‚Ä¢ Imported CSV data.
        
        ‚Ä¢ Ran queries with DQL.sql to validate relationships.
    
    4. Transform and Migrate Data to MongoDB:
    
        ‚Ä¢ Converted CSV files to JSON format for MongoDB compatibility.
        
        ‚Ä¢ Imported JSON files into MongoDB Atlas.

    5. Implemented Scheduled Data Synchronization
        
        ‚Ä¢ Export new PostgreSQL data as CSV.
        
        ‚Ä¢ Convert it to JSON automatically.
        
        ‚Ä¢ Import it into MongoDB at a set interval of 5 minutes.

üìÑ **Results & Final Verification**

    ‚úÖ PostgreSQL database successfully created.
    
    ‚úÖ Synthetic data generated & formatted for migration.
    
    ‚úÖ MongoDB collections accurately imported.
    
    ‚úÖ MongoDB queries returning expected results. 
    
    ‚úÖ GitHub repository ready for submission.

üìù Notes

    ‚è∫ Python (faker) was used to generate realistic streaming service data.
    
    ‚è∫ CSV files were formatted before conversion to JSON.
    
    ‚è∫ IP Whitelisting was essential for MongoDB Atlas access.
